# -*- coding: utf-8 -*-
"""US crime prediction

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17TXTsaikmazk17UtHnUeCG7jKnUD8lO5

# Dependencies
"""

!pip install geopandas

!pip install contextily

!pip install geoplot

!pip install eli5

!pip install pdpbox

!!pip install pip --upgrade

!pip install shap

import pandas as pd
from shapely.geometry import  Point
import geopandas as gpd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
import seaborn as sns
from matplotlib import cm
import urllib.request
import shutil
import zipfile
import os
import re
import contextily as ctx
import geoplot as gplt
import lightgbm as lgb
import eli5
from eli5.sklearn import PermutationImportance
from lightgbm import LGBMClassifier
from matplotlib import pyplot as plt
# from pdpbox import pdp, get_dataset, info_plots
import shap
from wordcloud import WordCloud, STOPWORDS
from sklearn.metrics import classification_report
from sklearn.ensemble import GradientBoostingClassifier

"""# Loading the data"""

# data in the dataset is already split into training and testing
# reading the data

train_data = pd.read_csv('/content/drive/MyDrive/datasets/US crime/train.csv')
test_data = pd.read_csv('/content/drive/MyDrive/datasets/US crime/test.csv')

#sample data
train_data.head()

#overview of train data
train_data.info()

"""There are no null values in the data

**Description of columns: **   
Dates - timestamp of the crime incident.   
Category - category of the crime incident. (This is our target variable.).   
Descript - detailed description of the crime incident.    
DayOfWeek - the day of the week.    
PdDistrict - the name of the Police Department District.    
Resolution - The resolution of the crime incident.     
Address - the approximate street address of the crime incident.    
X - Longitude.    
Y - Latitude.

# Data cleaning
"""

# checking null values in the test data
test_data.isnull().sum()

"""No null values in the test data as well"""

# checking the duplicate values in the train data
train_data.duplicated().sum()

"""There are 2323 duplicate values in the data

"""

#dropping the duplicate values
train_data.drop_duplicates(inplace=True)

"""# Exploring the data"""

train_data['Category'].unique()

plt.figure(figsize=(10,15))
sns.countplot(y='Category',data=train_data)
plt.show()

def create_gdf(df):
    gdf = df.copy()
    gdf['Coordinates'] = list(zip(gdf.X, gdf.Y))
    gdf.Coordinates = gdf.Coordinates.apply(Point)
    gdf = gpd.GeoDataFrame(
        gdf, geometry='Coordinates', crs={'init': 'epsg:4326'})
    return gdf

train_gdf = create_gdf(train_data)

world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))
ax = world.plot(color='white', edgecolor='black')
train_gdf.plot(ax=ax, color='red')
plt.show()

data = train_data.groupby('DayOfWeek').count().iloc[:, 0]
data = data.reindex([
    'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday',
    'Sunday'
])

plt.figure(figsize=(10, 5))
with sns.axes_style("white"):
    ax = sns.barplot(
        x=data.index, y=(data.values / data.values.sum()) * 100,
        orient='v',
        palette=cm.ScalarMappable(cmap='flag').to_rgba(data.values))

plt.title('Incidents per Weekday', fontdict={'fontsize': 16})
plt.xlabel('Weekday')
plt.ylabel('Incidents (%)')

plt.show()

# description of the crimes
train_data['Descript'].values[:10]

#resolution of the crimes
train_data['Resolution'].values[:10]

"""# Selecting top 5 crimes"""

#slicing the dataset with major crimes
train_data['Category'].value_counts()[:5]

"""Top 10 crimes are shown above"""

train_data.columns

major_crimes=['LARCENY/THEFT','OTHER OFFENSES','NON-CRIMINAL','ASSAULT','DRUG/NARCOTIC',
           'VEHICLE THEFT','VANDALISM','WARRANTS','BURGLARY','SUSPICIOUS OCC']
major_crimes_5=['LARCENY/THEFT','OTHER OFFENSES','NON-CRIMINAL','ASSAULT','DRUG/NARCOTIC']

data_new = train_data.loc[train_data['Category'].isin(major_crimes_5)]

data_new['Category'].unique()

data_new.info()

"""# Data preprocessing"""

#converting the string column to date column
data_new['Dates'] = pd.to_datetime(data_new['Dates'])

def feature_engineering(data):
  #this function divides the date column into date,day,month, hours and minutes
  #and divides the address also 
    data['Date'] = data['Dates'].dt.date #seperating the date from date column
    data['n_days'] = (
        data['Date'] - data['Date'].min()).apply(lambda x: x.days)
    data['Day'] = data['Dates'].dt.day
    data['DayOfWeek'] = data['Dates'].dt.weekday
    data['Month'] = data['Dates'].dt.month
    data['Year'] = data['Dates'].dt.year
    data['Hour'] = data['Dates'].dt.hour
    data['Minute'] = data['Dates'].dt.minute
    data['Block'] = data['Address'].str.contains('block', case=False)
    
    data.drop(columns=['Dates','Date','Address'], inplace=True) #removing the date,dates and adress columns
        
    return data

data_new = feature_engineering(data_new)
data_new.drop(columns=['Descript','Resolution'], inplace=True)

#data after preprocessing
data_new.head()

#categorical columns encoding
le1 = LabelEncoder()
data_new['PdDistrict'] = le1.fit_transform(data_new['PdDistrict'])
le2 = LabelEncoder()
y = le2.fit_transform(data_new.pop('Category'))

train_X, val_X, train_y, val_y = train_test_split(data_new, y)

model =LGBMClassifier(objective='multiclass', num_class=5).fit(train_X, train_y)

perm = PermutationImportance(model).fit(val_X, val_y)
eli5.show_weights(perm, feature_names=val_X.columns.tolist())

predictions = model.predict(val_X)
predictions

print(classification_report(val_y,predictions))

"""# Hyper parameter tuning"""

params = {'boosting':'gbdt',
          'objective':'multiclass',
          'num_class':39,
          'max_delta_step':0.9,
          'min_data_in_leaf': 21,
          'learning_rate': 0.4,
          'max_bin': 465,
          'num_leaves': 41
         }

model = LGBMClassifier(**params).fit(train_X, train_y, categorical_feature=['PdDistrict'])

predictions = model.predict(val_X)

print(classification_report(val_y,predictions))

"""Gradient boosting"""

gb_classifier = GradientBoostingClassifier()
gb_classifier.fit(train_X, train_y)

gb_pred = gb_classifier.predict(val_X)

print(classification_report(val_y,gb_pred))

"""Top 10 crimes"""

